{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Fraudulent Credit Card Transaction with NN\n",
    "\n",
    "Inspiration  \n",
    "Identify fraudulent credit card transactions.  \n",
    "https://www.kaggle.com/mlg-ulb/creditcardfraud/home\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load and explore data\n",
    "\n",
    "#### Data Description \n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "Based on data description, we know that v1-v28 are PCA components, they are all numerical variables, so we do not have categorical variables to encode. And here we are going to build a NN, we do not need take care of feature correlation problem (ref). But, NN is very sensitive to feature scaling, here we need normalize the Amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Normalized_Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data = data.drop(['Time','Amount'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance\n",
    "\n",
    "As pointed out, the data is highly unbalanced, with only 492 frauds out of 284,807 transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a62f54ea20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(\"Class\",data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define model performance\n",
    "\n",
    "For such unbalanced classification problem, we need define a proper metric. In this case, we want to detect as many frauds as possible, because for a bank, the cost of identifying a fraud as a normal transaction is very expensive. Therefore, we want use recall, precision or ROUARU intead of accuracy. \n",
    "\n",
    "Confusion matrix : also known as the error matrix, allows visualization of the performance of an algorithm :  \n",
    "\n",
    "true positive (TP) : Diabetic correctly identified as diabetic  \n",
    "true negative (TN) : Healthy correctly identified as healthy  \n",
    "false positive (FP) : Healthy incorrectly identified as diabetic  \n",
    "false negative (FN) : Diabetic incorrectly identified as healthy  \n",
    "\n",
    "Metrics :  \n",
    "\n",
    "Accuracy : (TP +TN) / (TP + TN + FP +FN)  \n",
    "Precision : TP / (TP + FP)  \n",
    "Recall : TP / (TP + FN)  \n",
    "F1 score : 2 x ((Precision x Recall) / (Precision + Recall))  \n",
    "\n",
    "Roc Curve :   \n",
    "\n",
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(y_test, y_pred) :\n",
    "    \n",
    "     # Confusion matrix\n",
    "    CM = confusion_matrix(y_test, y_pred)\n",
    "    # Get true positives(tp), false positives(fp), false negatives(fn)\n",
    "    tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "\n",
    "    # Prediction report\n",
    "    sns.heatmap(CM,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n",
    "    plt.title(\"Confusion_matrix\")\n",
    "    plt.xlabel(\"Predicted_class\")\n",
    "    plt.ylabel(\"Real class\")\n",
    "    plt.show()\n",
    "    print(\"\\n----------Classification Report------------------------------------\")\n",
    "    print(classification_report(y_test.values, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data\n",
    "\n",
    "A common solution for imbalanced classification problem is under sampling or over sampling. Here I tested under sampling, to make the algorithm run faster too. Basically, I choose equall amount of normal transactions as the fraud transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions train dataset:  688\n",
      "Number transactions test dataset:  296\n"
     ]
    }
   ],
   "source": [
    "number_of_fraud = len(data[data.Class == 1])\n",
    "number_of_normal= len(data[data.Class == 0])\n",
    "\n",
    "#indices of normal class\n",
    "indices_of_normal = data[data.Class==0].index\n",
    "#randomly choose same amount of samples as the fraud, and return their indices\n",
    "random_indices_of_normal = np.array(np.random.choice(indices_of_normal, number_of_fraud, replace=False))\n",
    "#indices of fraud class\n",
    "indices_of_fraud = np.array(data[data.Class == 1].index)\n",
    "#indices of undersampled dataset\n",
    "indices_of_undersampled = np.concatenate([random_indices_of_normal, indices_of_fraud])\n",
    "#undersampled dataset)\n",
    "data_of_undersampled = data.iloc[indices_of_undersampled,:]\n",
    "\n",
    "#undersampled dataset\n",
    "X_undersampled = data_of_undersampled.loc[:,data_of_undersampled.columns!='Class']\n",
    "y_undersampled = data_of_undersampled.loc[:,data_of_undersampled.columns=='Class']\n",
    "\n",
    "#train and test dataset splitted from undersampled dataset, with 70/30 ratio\n",
    "X_train_undersampled, X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_undersampled,y_undersampled,test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Number transactions train dataset: \", len(X_train_undersampled))\n",
    "print(\"Number transactions test dataset: \", len(X_test_undersampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGBM\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:  \n",
    "Faster training speed and higher efficiency.  \n",
    "Lower memory usage.  \n",
    "Better accuracy.  \n",
    "Support of parallel and GPU learning.  \n",
    "Capable of handling large-scale data.    \n",
    "https://lightgbm.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 161 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgbm.LGBMClassifier(random_state = 42)\n",
    "clf.fit(X_train_undersampled, y_train_undersampled.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       345\n",
      "           1       1.00      1.00      1.00       343\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       688\n",
      "   macro avg       1.00      1.00      1.00       688\n",
      "weighted avg       1.00      1.00      1.00       688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = clf.predict(X_train_undersampled)\n",
    "model_performance(y_train_undersampled, y_train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       147\n",
      "           1       0.99      0.89      0.94       149\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       296\n",
      "   macro avg       0.94      0.94      0.94       296\n",
      "weighted avg       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = clf.predict(X_test_undersampled)\n",
    "model_performance(y_test_undersampled, y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter optimization\n",
    "\n",
    "To find the best hyperparameters, we use random search.  \n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html \n",
    "\n",
    "important parameters:  \n",
    "1. num_leaves: This is the main parameter to control the complexity of the tree model.  \n",
    "2. min_data_in_leaf: This is a very important parameter to prevent over-fitting in a leaf-wise tree.  \n",
    "3. max_depth: to limit the tree depth explicitly.\n",
    "\n",
    "For Faster Speed  \n",
    "Use bagging by setting bagging_fraction and bagging_freq  \n",
    "Use feature sub-sampling by setting feature_fraction  \n",
    "Use small max_bin  \n",
    "Use save_binary to speed up data loading in future learning  \n",
    "Use parallel learning, refer to Parallel Learning Guide  \n",
    "\n",
    "For Better Accuracy  \n",
    "Use large max_bin (may be slower)  \n",
    "Use small learning_rate with large num_iterations  \n",
    "Use large num_leaves (may cause over-fitting)  \n",
    "Use bigger training data  \n",
    "Try dart  \n",
    "\n",
    "Deal with Over-fitting  \n",
    "Use small max_bin  \n",
    "Use small num_leaves  \n",
    "Use min_data_in_leaf and min_sum_hessian_in_leaf  \n",
    "Use bagging by set bagging_fraction and bagging_freq  \n",
    "Use feature sub-sampling by set feature_fraction  \n",
    "Use bigger training data  \n",
    "Try lambda_l1, lambda_l2 and min_gain_to_split for regularization  \n",
    "Try max_depth to avoid growing deep tree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LGBMClassifier in module lightgbm.sklearn:\n",
      "\n",
      "class LGBMClassifier(LGBMModel, sklearn.base.ClassifierMixin)\n",
      " |  LightGBM classifier.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LGBMClassifier\n",
      " |      LGBMModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_class_weight=None, eval_init_score=None, eval_metric=None, early_stopping_rounds=None, verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None)\n",
      " |      Build a gradient boosting model from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input feature matrix.\n",
      " |      y : array-like of shape = [n_samples]\n",
      " |          The target values (class labels in classification, real numbers in regression).\n",
      " |      sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n",
      " |          Weights of training data.\n",
      " |      init_score : array-like of shape = [n_samples] or None, optional (default=None)\n",
      " |          Init score of training data.\n",
      " |      group : array-like or None, optional (default=None)\n",
      " |          Group data of training data.\n",
      " |      eval_set : list or None, optional (default=None)\n",
      " |          A list of (X, y) tuple pairs to use as validation sets.\n",
      " |      eval_names : list of strings or None, optional (default=None)\n",
      " |          Names of eval_set.\n",
      " |      eval_sample_weight : list of arrays or None, optional (default=None)\n",
      " |          Weights of eval data.\n",
      " |      eval_class_weight : list or None, optional (default=None)\n",
      " |          Class weights of eval data.\n",
      " |      eval_init_score : list of arrays or None, optional (default=None)\n",
      " |          Init score of eval data.\n",
      " |      eval_group : list of arrays or None, optional (default=None)\n",
      " |          Group data of eval data.\n",
      " |      eval_metric : string, list of strings, callable or None, optional (default=None)\n",
      " |          If string, it should be a built-in evaluation metric to use.\n",
      " |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      " |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      " |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      " |      early_stopping_rounds : int or None, optional (default=None)\n",
      " |          Activates early stopping. The model will train until the validation score stops improving.\n",
      " |          Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      " |          to continue training.\n",
      " |          Requires at least one validation data and one metric.\n",
      " |          If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      " |      verbose : bool or int, optional (default=True)\n",
      " |          Requires at least one evaluation data.\n",
      " |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      " |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      " |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      " |      \n",
      " |          Example\n",
      " |          -------\n",
      " |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      " |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      " |      \n",
      " |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      " |          Feature names.\n",
      " |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      " |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      " |          Categorical features.\n",
      " |          If list of int, interpreted as indices.\n",
      " |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      " |          If 'auto' and data is pandas DataFrame, pandas categorical columns are used.\n",
      " |          All values in categorical features should be less than int32 max value (2147483647).\n",
      " |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      " |          All negative values in categorical features will be treated as missing values.\n",
      " |      callbacks : list of callback functions or None, optional (default=None)\n",
      " |          List of callback functions that are applied at each iteration.\n",
      " |          See Callbacks in Python API for more information.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      Custom eval function expects a callable with following signatures:\n",
      " |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      " |      ``func(y_true, y_pred, weight, group)``\n",
      " |      and returns (eval_name, eval_result, is_bigger_better) or\n",
      " |      list of (eval_name, eval_result, is_bigger_better):\n",
      " |      \n",
      " |          y_true : array-like of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |          weight : array-like of shape = [n_samples]\n",
      " |              The weight of samples.\n",
      " |          group : array-like\n",
      " |              Group/query data, used for ranking task.\n",
      " |          eval_name : string\n",
      " |              The name of evaluation.\n",
      " |          eval_result : float\n",
      " |              The eval result.\n",
      " |          is_bigger_better : bool\n",
      " |              Is eval result bigger better, e.g. AUC is bigger_better.\n",
      " |      \n",
      " |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      " |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n",
      " |  \n",
      " |  predict(self, X, raw_score=False, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      " |      Return the predicted value for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      raw_score : bool, optional (default=False)\n",
      " |          Whether to predict raw scores.\n",
      " |      num_iteration : int or None, optional (default=None)\n",
      " |          Limit number of iterations in the prediction.\n",
      " |          If None, if the best iteration exists, it is used; otherwise, all trees are used.\n",
      " |          If <= 0, all trees are used (no limits).\n",
      " |      pred_leaf : bool, optional (default=False)\n",
      " |          Whether to predict leaf index.\n",
      " |      pred_contrib : bool, optional (default=False)\n",
      " |          Whether to predict feature contributions.\n",
      " |      \n",
      " |          Note\n",
      " |          ----\n",
      " |          If you want to get more explanation for your model's predictions using SHAP values\n",
      " |          like SHAP interaction values,\n",
      " |          you can install shap package (https://github.com/slundberg/shap).\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other parameters for the prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      " |          The predicted values.\n",
      " |      X_leaves : array-like of shape = [n_samples, n_trees] or shape [n_samples, n_trees * n_classes]\n",
      " |          If ``pred_leaf=True``, the predicted leaf every tree for each sample.\n",
      " |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape [n_samples, (n_features + 1) * n_classes]\n",
      " |          If ``pred_contrib=True``, the each feature contributions for each sample.\n",
      " |  \n",
      " |  predict_proba(self, X, raw_score=False, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      " |      Return the predicted probability for each class for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      raw_score : bool, optional (default=False)\n",
      " |          Whether to predict raw scores.\n",
      " |      num_iteration : int or None, optional (default=None)\n",
      " |          Limit number of iterations in the prediction.\n",
      " |          If None, if the best iteration exists, it is used; otherwise, all trees are used.\n",
      " |          If <= 0, all trees are used (no limits).\n",
      " |      pred_leaf : bool, optional (default=False)\n",
      " |          Whether to predict leaf index.\n",
      " |      pred_contrib : bool, optional (default=False)\n",
      " |          Whether to predict feature contributions.\n",
      " |      \n",
      " |          Note\n",
      " |          ----\n",
      " |          If you want to get more explanation for your model's predictions using SHAP values\n",
      " |          like SHAP interaction values,\n",
      " |          you can install shap package (https://github.com/slundberg/shap).\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other parameters for the prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_probability : array-like of shape = [n_samples, n_classes]\n",
      " |          The predicted probability for each class for each sample.\n",
      " |      X_leaves : array-like of shape = [n_samples, n_trees * n_classes]\n",
      " |          If ``pred_leaf=True``, the predicted leaf every tree for each sample.\n",
      " |      X_SHAP_values : array-like of shape = [n_samples, (n_features + 1) * n_classes]\n",
      " |          If ``pred_contrib=True``, the each feature contributions for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |      Get the class label array.\n",
      " |  \n",
      " |  n_classes_\n",
      " |      Get the number of classes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LGBMModel:\n",
      " |  \n",
      " |  __init__(self, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      " |      Construct a gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      boosting_type : string, optional (default='gbdt')\n",
      " |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      " |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      " |          'goss', Gradient-based One-Side Sampling.\n",
      " |          'rf', Random Forest.\n",
      " |      num_leaves : int, optional (default=31)\n",
      " |          Maximum tree leaves for base learners.\n",
      " |      max_depth : int, optional (default=-1)\n",
      " |          Maximum tree depth for base learners, -1 means no limit.\n",
      " |      learning_rate : float, optional (default=0.1)\n",
      " |          Boosting learning rate.\n",
      " |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      " |          in training using ``reset_parameter`` callback.\n",
      " |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      " |      n_estimators : int, optional (default=100)\n",
      " |          Number of boosted trees to fit.\n",
      " |      subsample_for_bin : int, optional (default=200000)\n",
      " |          Number of samples for constructing bins.\n",
      " |      objective : string, callable or None, optional (default=None)\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      " |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      " |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |          Use this parameter only for multi-class classification task;\n",
      " |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      " |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      " |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |          If None, all classes are supposed to have weight one.\n",
      " |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      " |          if ``sample_weight`` is specified.\n",
      " |      min_split_gain : float, optional (default=0.)\n",
      " |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |      min_child_weight : float, optional (default=1e-3)\n",
      " |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      " |      min_child_samples : int, optional (default=20)\n",
      " |          Minimum number of data needed in a child (leaf).\n",
      " |      subsample : float, optional (default=1.)\n",
      " |          Subsample ratio of the training instance.\n",
      " |      subsample_freq : int, optional (default=0)\n",
      " |          Frequence of subsample, <=0 means no enable.\n",
      " |      colsample_bytree : float, optional (default=1.)\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      reg_alpha : float, optional (default=0.)\n",
      " |          L1 regularization term on weights.\n",
      " |      reg_lambda : float, optional (default=0.)\n",
      " |          L2 regularization term on weights.\n",
      " |      random_state : int or None, optional (default=None)\n",
      " |          Random number seed.\n",
      " |          If None, default seeds in C++ code will be used.\n",
      " |      n_jobs : int, optional (default=-1)\n",
      " |          Number of parallel threads.\n",
      " |      silent : bool, optional (default=True)\n",
      " |          Whether to print messages while running boosting.\n",
      " |      importance_type : string, optional (default='split')\n",
      " |          The type of feature importance to be filled into ``feature_importances_``.\n",
      " |          If 'split', result contains numbers of times the feature is used in a model.\n",
      " |          If 'gain', result contains total gains of splits which use the feature.\n",
      " |      **kwargs\n",
      " |          Other parameters for the model.\n",
      " |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      " |      \n",
      " |          Note\n",
      " |          ----\n",
      " |          \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      n_features_ : int\n",
      " |          The number of features of fitted model.\n",
      " |      classes_ : array of shape = [n_classes]\n",
      " |          The class label array (only for classification problem).\n",
      " |      n_classes_ : int\n",
      " |          The number of classes (only for classification problem).\n",
      " |      best_score_ : dict or None\n",
      " |          The best score of fitted model.\n",
      " |      best_iteration_ : int or None\n",
      " |          The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      " |      objective_ : string or callable\n",
      " |          The concrete objective used while fitting this model.\n",
      " |      booster_ : Booster\n",
      " |          The underlying Booster of this model.\n",
      " |      evals_result_ : dict or None\n",
      " |          The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      " |      feature_importances_ : array of shape = [n_features]\n",
      " |          The feature importances (the higher, the more important the feature).\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      A custom objective function can be provided for the ``objective`` parameter.\n",
      " |      In this case, it should have the signature\n",
      " |      ``objective(y_true, y_pred) -> grad, hess`` or\n",
      " |      ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      " |      \n",
      " |          y_true : array-like of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |          group : array-like\n",
      " |              Group/query data, used for ranking task.\n",
      " |          grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The value of the gradient for each sample point.\n",
      " |          hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The value of the second derivative for each sample point.\n",
      " |      \n",
      " |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      " |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
      " |      and you should group grad and hess in this way as well.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, optional (default=True)\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params\n",
      " |          Parameter names with their new values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from LGBMModel:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |      Get the best iteration of fitted model.\n",
      " |  \n",
      " |  best_score_\n",
      " |      Get the best score of fitted model.\n",
      " |  \n",
      " |  booster_\n",
      " |      Get the underlying lightgbm Booster of this model.\n",
      " |  \n",
      " |  evals_result_\n",
      " |      Get the evaluation results.\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Get feature importances.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      Feature importance in sklearn interface used to normalize to 1,\n",
      " |      it's deprecated after 2.0.4 and is the same as Booster.feature_importance() now.\n",
      " |      ``importance_type`` attribute is passed to the function\n",
      " |      to configure the type of importance values to be extracted.\n",
      " |  \n",
      " |  n_features_\n",
      " |      Get the number of features of fitted model.\n",
      " |  \n",
      " |  objective_\n",
      " |      Get the concrete objective used while fitting this model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lgbm.LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "              'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, 5000],\n",
    "              'num_leaves': sp_randint(6, 50), \n",
    "              'min_child_samples': sp_randint(100, 500), \n",
    "              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "              'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "              'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "fit_params = {\"early_stopping_rounds\" : 50, \n",
    "             \"eval_metric\" : 'binary', \n",
    "             \"eval_set\" : [(X_test_undersampled,y_test_undersampled)],\n",
    "             'eval_names': ['valid'],\n",
    "             'verbose': 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomizedSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class RandomizedSearchCV(BaseSearchCV)\n",
      " |  Randomized search on hyper parameters.\n",
      " |  \n",
      " |  RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated search over parameter settings.\n",
      " |  \n",
      " |  In contrast to GridSearchCV, not all parameter values are tried out, but\n",
      " |  rather a fixed number of parameter settings is sampled from the specified\n",
      " |  distributions. The number of parameter settings that are tried is\n",
      " |  given by n_iter.\n",
      " |  \n",
      " |  If all parameters are presented as a list,\n",
      " |  sampling without replacement is performed. If at least one parameter\n",
      " |  is given as a distribution, sampling with replacement is used.\n",
      " |  It is highly recommended to use continuous distributions for continuous\n",
      " |  parameters.\n",
      " |  \n",
      " |  Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not\n",
      " |  accept a custom RNG instance and always use the singleton RNG from\n",
      " |  ``numpy.random``. Hence setting ``random_state`` will not guarantee a\n",
      " |  deterministic iteration whenever ``scipy.stats`` distributions are used to\n",
      " |  define the parameter search space.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      A object of that type is instantiated for each grid point.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_distributions : dict\n",
      " |      Dictionary with parameters names (string) as keys and distributions\n",
      " |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      " |      method for sampling (such as those from scipy.stats.distributions).\n",
      " |      If a list is given, it is sampled uniformly.\n",
      " |  \n",
      " |  n_iter : int, default=10\n",
      " |      Number of parameter settings that are sampled. n_iter trades\n",
      " |      off runtime vs quality of the solution.\n",
      " |  \n",
      " |  scoring : string, callable, list/tuple, dict or None, default: None\n",
      " |      A single string (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's default scorer (if available) is used.\n",
      " |  \n",
      " |  fit_params : dict, optional\n",
      " |      Parameters to pass to the fit method.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``fit_params`` as a constructor argument was deprecated in version\n",
      " |         0.19 and will be removed in version 0.21. Pass fit parameters to\n",
      " |         the ``fit`` method instead.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  pre_dispatch : int, or string, optional\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A string, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : boolean, default='warn'\n",
      " |      If True, return the average score across folds, weighted by the number\n",
      " |      of samples in each test set. In this case, the data is assumed to be\n",
      " |      identically distributed across the folds, and the loss minimized is\n",
      " |      the total loss per sample, and not the mean loss across the folds. If\n",
      " |      False, return the average score across folds. Default is True, but\n",
      " |      will change to False in version 0.21, to correspond to the standard\n",
      " |      definition of cross-validation.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          Parameter ``iid`` will change from True to False by default in\n",
      " |          version 0.22, and will be removed in 0.24.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, optional\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 3-fold cross validation,\n",
      " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          ``cv`` default value if None will change from 3-fold to 5-fold\n",
      " |          in v0.22.\n",
      " |  \n",
      " |  refit : boolean, or string default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a string denoting the\n",
      " |      scorer that would be used to find the best parameters for refitting\n",
      " |      the estimator at the end.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``RandomizedSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_params_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      Pseudo random number generator state used for random uniform sampling\n",
      " |      from lists of possible values instead of scipy.stats distributions.\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  error_score : 'raise' or numeric\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error. Default is 'raise' but from\n",
      " |      version 0.22 it will change to np.nan.\n",
      " |  \n",
      " |  return_train_score : boolean, optional\n",
      " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |  \n",
      " |      Current default is ``'warn'``, which behaves as ``True`` in addition\n",
      " |      to raising a warning when a training score is looked up.\n",
      " |      That default will be changed to ``False`` in 0.21.\n",
      " |      Computing training scores is used to get insights on how different\n",
      " |      parameter settings impact the overfitting/underfitting trade-off.\n",
      " |      However computing the scores on the training set can be computationally\n",
      " |      expensive and is not strictly required to select the parameters that\n",
      " |      yield the best generalization performance.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n",
      " |      +==============+=============+===================+===+===============+\n",
      " |      |    'rbf'     |     0.1     |       0.80        |...|       2       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      |    'rbf'     |     0.2     |       0.90        |...|       1       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      |    'rbf'     |     0.3     |       0.70        |...|       1       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n",
      " |                                        mask = False),\n",
      " |          'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n",
      " |          'split0_test_score'  : [0.80, 0.90, 0.70],\n",
      " |          'split1_test_score'  : [0.82, 0.50, 0.70],\n",
      " |          'mean_test_score'    : [0.81, 0.70, 0.70],\n",
      " |          'std_test_score'     : [0.01, 0.20, 0.00],\n",
      " |          'rank_test_score'    : [3, 1, 1],\n",
      " |          'split0_train_score' : [0.80, 0.92, 0.70],\n",
      " |          'split1_train_score' : [0.82, 0.55, 0.70],\n",
      " |          'mean_train_score'   : [0.81, 0.74, 0.70],\n",
      " |          'std_train_score'    : [0.01, 0.19, 0.00],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01],\n",
      " |          'mean_score_time'    : [0.01, 0.06, 0.04],\n",
      " |          'std_score_time'     : [0.00, 0.00, 0.00],\n",
      " |          'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator or dict\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute is present only if\n",
      " |      ``refit`` is specified.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  refit_time_ : float\n",
      " |      Seconds used for refitting the best model on the whole dataset.\n",
      " |  \n",
      " |      This is present only if ``refit`` is not False.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The parameters selected are those that maximize the score of the held-out\n",
      " |  data, according to the scoring parameter.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  parameter setting(and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  :class:`GridSearchCV`:\n",
      " |      Does exhaustive search over a grid of parameters.\n",
      " |  \n",
      " |  :class:`ParameterSampler`:\n",
      " |      A generator over parameter settings, constructed from\n",
      " |      param_distributions.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomizedSearchCV\n",
      " |      BaseSearchCV\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_distributions, n_iter=10, scoring=None, fit_params=None, n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score='raise-deprecating', return_train_score='warn')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score reached: 0.9097127222982218 with params: {'colsample_bytree': 0.7380735222242458, 'learning_rate': 0.4, 'max_depth': 5, 'min_child_samples': 211, 'min_child_weight': 1e-05, 'n_estimators': 800, 'num_leaves': 49, 'reg_alpha': 1, 'reg_lambda': 50, 'subsample': 0.73223959752976} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "#number of combinations\n",
    "n_iter = 200 \n",
    "\n",
    "#intialize lgbm and lunch the search\n",
    "clf = lgbm.LGBMClassifier(random_state=43, silent=True, metric='None')\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_iter,\n",
    "    scoring='recall',\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=43,\n",
    "    verbose=True)\n",
    "\n",
    "grid_search.fit(X_train_undersampled, y_train_undersampled.values.ravel(), **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(grid_search.best_score_, grid_search.best_params_))\n",
    "\n",
    "opt_parameters =  grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       345\n",
      "           1       0.99      0.95      0.97       343\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       688\n",
      "   macro avg       0.97      0.97      0.97       688\n",
      "weighted avg       0.97      0.97      0.97       688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = grid_search.predict(X_train_undersampled)\n",
    "model_performance(y_train_undersampled, y_train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       147\n",
      "           1       0.98      0.90      0.94       149\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       296\n",
      "   macro avg       0.94      0.94      0.94       296\n",
      "weighted avg       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = grid_search.predict(X_test_undersampled)\n",
    "model_performance(y_test_undersampled, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
